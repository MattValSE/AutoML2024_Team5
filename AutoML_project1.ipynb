{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattValSE/AutoML2024_Team5/blob/main/AutoML_project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable Automated Machine Learning (LTAT.02.023)\n",
        "## Course project 1\n",
        "Authors:  Kristjan Laid, Mattias Väli and Evely Kirsiaed\n",
        "\n",
        "Description:\n",
        "1. Build and train a baseline, and record the result (train different machine learning algorithms with their default hyperparameters (random forest, decision tree,........etc.) and then select the one that achieves the best performance.).\n",
        "2. Based on the problem at hand, you study the potential pipeline structure, algorithms, or feature transformers at each step and hyperparameter ranges. Use hyperOpt with the potential search space to beat the baseline if possible.\n",
        "\n",
        "Assessment Criteria:\n",
        "1. The project’s code should be available and ready to run if needed.\n",
        "2. Everyone should understand the whole project and be ready to answer any question regarding it, not only the part they contributed to.\n",
        "3. The total assessment time is 15 minutes: 10 minutes for the presentation and 5 minutes for questions.\n",
        "4. You may be interrupted during the presentation for some questions.\n",
        "6. Your presentation should include a dataset description, search space configurations, used baseline, selected pipeline (autoML output), overtime monitoring of the process selection, comparison between the selected and baseline pipelines, statistical test results, and justification for each step.\n",
        "7. Evaluation criteria:\n",
        "\n",
        "a. Correctness of the code - 33% of the mark.\n",
        "\n",
        "b. Completeness of the presentation - 33% of the mark.\n",
        "\n",
        "c. Questions’ answers - 33% of the mark.\n",
        "\n",
        "8. All team members share the same mark for a and b and might get a different mark for c, based on each student’s answers."
      ],
      "metadata": {
        "id": "xf-8INoWYGPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset description\n",
        "todo"
      ],
      "metadata": {
        "id": "4SBPaUHg_HGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model\n"
      ],
      "metadata": {
        "id": "8hibnLOtZ_k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "rLxkhBrTaDAf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYOC8ombzBqX",
        "outputId": "b2788922-03da-45a8-e774-ddc7ba7811ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (e.g., Iris dataset)\n",
        "data = datasets.load_digits()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "cpTSLI06aId6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "id": "jzKV09dYYIef",
        "outputId": "599f2f65-2dc0-4c11-9e30-3e4cba92b463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "res = dict()\n",
        "for model in [GradientBoostingClassifier(random_state = 42),\n",
        "              RandomForestClassifier(random_state = 42),\n",
        "              XGBClassifier(random_state = 42),\n",
        "              RidgeClassifier(random_state = 42),\n",
        "              LogisticRegression(random_state = 42),\n",
        "              GaussianNB(),\n",
        "              BernoulliNB(),\n",
        "              NearestCentroid(),\n",
        "              KNeighborsClassifier(),\n",
        "              DecisionTreeClassifier(random_state = 42),\n",
        "              SVC()\n",
        "              ]:\n",
        "  # Perform cross-validation and calculate the mean accuracy for each baseline model\n",
        "  accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "  res[model] = accuracy\n",
        "\n",
        "print(\"Mean accuracy for baseline models\")\n",
        "#for el in dict(sorted(res.items(), key=lambda item: item[1], reverse=True)):\n",
        "#  print(f\"Cross-validated accuracy: {res[el]:.5f}, Model: {el}\")\n",
        "\n",
        "sorted_res = dict(sorted(res.items(), key=lambda item: item[1], reverse=True))\n",
        "for model_name, accuracy in sorted_res.items():\n",
        "    print(f\"\\\\item {model_name} - {accuracy * 100:.1f}\\\\%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTdwxGqjaay3",
        "outputId": "0cd5327e-08ff-4b2e-d200-8dcf6a1df1d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy for baseline models\n",
            "\\item SVC() - 94.9\\%\n",
            "\\item KNeighborsClassifier() - 94.4\\%\n",
            "\\item RandomForestClassifier(random_state=42) - 93.9\\%\n",
            "\\item GradientBoostingClassifier(random_state=42) - 92.2\\%\n",
            "\\item LogisticRegression(random_state=42) - 91.9\\%\n",
            "\\item XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
            "              num_parallel_tree=None, random_state=42, ...) - 91.8\\%\n",
            "\\item RidgeClassifier(random_state=42) - 88.8\\%\n",
            "\\item NearestCentroid() - 85.6\\%\n",
            "\\item BernoulliNB() - 85.3\\%\n",
            "\\item DecisionTreeClassifier(random_state=42) - 79.0\\%\n",
            "\\item GaussianNB() - 76.9\\%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter optimization"
      ],
      "metadata": {
        "id": "9CwqKKeC84bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter optimization using Hyperopt"
      ],
      "metadata": {
        "id": "7E3bZRfLGWmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Random search"
      ],
      "metadata": {
        "id": "lcykxbyQBUiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from hyperopt.pyll.base import scope"
      ],
      "metadata": {
        "id": "HhwTiZJNGaBi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the objective function to minimize\n",
        "def objective(params):\n",
        "    # Create the model using parameters from Hyperopt\n",
        "    rf = RandomForestClassifier(n_estimators=int(params['n_estimators']),\n",
        "                                criterion='gini',\n",
        "                              #  max_depth=int(params['max_depth']),\n",
        "                             #   min_samples_split=int(params['min_samples_split']),\n",
        "                                #min_samples_leaf=int(params['min_samples_leaf']),\n",
        "                                random_state=42)\n",
        "\n",
        "    # Fit the model on the training set\n",
        "    #rf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    #y_pred = rf.predict(X_valid)\n",
        "\n",
        "    # Compute the Mean Absolute Error (MAE)\n",
        "    #mae = mean_absolute_error(y_valid, y_pred)\n",
        "    accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "    # Return a dictionary with the loss (to minimize) and status\n",
        "    return {'loss': -accuracy,'status': STATUS_OK}\n",
        "\n",
        "# 3. Define the search space for Hyperopt\n",
        "search_space = {\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 30, 250, 5)),\n",
        "  #  'max_depth': scope.int(hp.quniform('max_depth', 2, 20, 1)),  # Integer values between 2 and 20\n",
        "  #  'min_samples_split': scope.int(hp.quniform('min_samples_split', 1, 50, 1)),\n",
        "  #  'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 50, 1))  # Integer values between 1 and 50\n",
        "}\n",
        "\n",
        "\n",
        "trials = Trials()\n",
        "# 5. Run Hyperopt to minimize the objective function\n",
        "best = fmin(fn=objective,                # Objective function\n",
        "            space=search_space,          # Search space\n",
        "            algo=tpe.suggest,            # Tree-structured Parzen Estimator (TPE) algorithm\n",
        "            max_evals=100,               # Number of evaluations\n",
        "            trials=trials,               # Store results\n",
        "            rstate=np.random.default_rng(42))  # Ensure reproducibility with a fixed random seed\n",
        "\n",
        "# 6. Print the best hyperparameters\n",
        "print(\"Best hyperparameters found: \", best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rH8Dr9mGeJ6",
        "outputId": "fd1019f8-a29f-4cab-b999-330cc685ad04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.10trial/s, best loss: -0.9493624264933457]\n",
            "Best hyperparameters found:  {'n_estimators': 210.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train a model with the best hyperparameters on the full training set and evaluate on the test set\n",
        "\n",
        "best_rf = RandomForestClassifier(n_estimators=int(best['n_estimators']),\n",
        "                                 criterion='gini',\n",
        "#                                 max_depth=int(best['max_depth']),\n",
        "#                                 min_samples_split=int(best['min_samples_split']),\n",
        "#                                 min_samples_leaf=int(best['min_samples_leaf']),\n",
        "                                 random_state=42)\n",
        "\n",
        "#best_rf = RandomForestClassifier(random_state=42)\n",
        "accuracy = cross_val_score(best_rf, X, y, cv=5, scoring='accuracy') .mean()\n",
        "print(f\"Cross-validated accuracy: {accuracy:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nafXOcD69QsZ",
        "outputId": "ba17984f-fe99-4da5-bf18-5bf1cdad032f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy: 0.94104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Grid search"
      ],
      "metadata": {
        "id": "-b0RvrRjBgkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "n_estimators_values = np.arange(30, 250, 20)\n",
        "max_depth_values = np.arange(1, 20, 5)\n",
        "min_sampl_values = np.arange(1, 20, 5)\n",
        "min_sampl_leaf = np.arange(1, 20, 5)\n",
        "\n",
        "grid_search_space = {\n",
        "    'n_estimators': hp.choice('n_estimators', n_estimators_values),\n",
        "    'max_depth': hp.choice('max_depth', max_depth_values),  # Grid of fixed values\n",
        "    'min_samples_split': hp.choice('min_samples_split', min_sampl_values),\n",
        "    'min_samples_leaf': hp.choice('min_samples_leaf', min_sampl_leaf)  # Integer values between 1 and 50\n",
        "}\n",
        "# Use itertools.product to find the Cartesian product (i.e., all possible combinations)\n",
        "all_combinations = list(itertools.product(n_estimators_values, max_depth_values,min_sampl_values,min_sampl_leaf))\n",
        "total_combinations = len(all_combinations)\n",
        "# Perform Grid Search using fmin\n",
        "best_grid = fmin(fn=objective,        # Objective function\n",
        "                 space=grid_search_space,  # Grid search space\n",
        "                 algo=tpe.suggest,    # Still use TPE, but this is effectively Grid Search due to the fixed values\n",
        "                 max_evals=total_combinations,       # Number of evaluations, adjust if necessary\n",
        "                 rstate=np.random.default_rng(42))  # Ensure reproducibility\n",
        "best_grid_rf = RandomForestClassifier(n_estimators=int(best_grid['n_estimators']),\n",
        "                                      max_depth=int(best_grid['max_depth']),\n",
        "                                      min_samples_split=int(best_grid['min_samples_split']),\n",
        "                                      min_samples_leaf=int(best_grid['min_samples_leaf']),\n",
        "                                      random_state=42,\n",
        "                                      criterion='gini')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGDg-wphBloF",
        "outputId": "a980f138-797d-4810-d71b-a93adfe69d5a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 704/704 [05:35<00:00,  2.10trial/s, best loss: -0.9493624264933457]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the actual values from the search space using the indices stored in best_grid\n",
        "best_n_estimators = n_estimators_values[int(best_grid['n_estimators'])]\n",
        "best_max_depth = max_depth_values[int(best_grid['max_depth'])]\n",
        "best_min_samples_split = min_sampl_values[int(best_grid['min_samples_split'])]\n",
        "best_min_samples_leaf = min_sampl_leaf[int(best_grid['min_samples_leaf'])]\n",
        "\n",
        "# Now create the RandomForestClassifier with the correct hyperparameter values\n",
        "best_grid_rf = RandomForestClassifier(n_estimators=best_n_estimators,\n",
        "                                      max_depth=best_max_depth,\n",
        "                                      min_samples_split=best_min_samples_split,\n",
        "                                      min_samples_leaf=best_min_samples_leaf,\n",
        "                                      random_state=42,\n",
        "                                      criterion='gini')"
      ],
      "metadata": {
        "id": "sxFXBCHRn4Fz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = cross_val_score(best_grid_rf, X, y, cv=5, scoring='accuracy') .mean()\n",
        "print(f\"Cross-validated accuracy: {accuracy:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNrqV-6pDh8o",
        "outputId": "7a8b308f-861a-4e5a-8672-8dd860b6497c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy: 0.90319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian optimization"
      ],
      "metadata": {
        "id": "LTyJQBSSDu3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "from skopt.space import Real\n",
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NxN73Xy9-J6",
        "outputId": "8dea5dbe-f5f6-4a28-c069-84ccec34384f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt.space import Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skopt import gp_minimize\n",
        "\n",
        "# Define the SVM model's hyperparameter space\n",
        "space = [\n",
        "    Real(1e-6, 100.0, \"log-uniform\", name='C'),\n",
        "    Real(1e-6, 100.0, \"log-uniform\", name='gamma'),\n",
        "    Categorical(['linear', 'rbf', 'poly', 'sigmoid'], name='kernel')\n",
        "]\n",
        "\n",
        "# Define the objective function to minimize\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    # Create the SVM model with the given hyperparameters\n",
        "    model = SVC(C=params['C'], gamma=params['gamma'], kernel=params['kernel'])\n",
        "\n",
        "    # Perform cross-validation and calculate the mean accuracy\n",
        "    accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "    # Return the negative accuracy (because we want to minimize the objective)\n",
        "    return -accuracy\n",
        "\n",
        "# Run Bayesian optimization to find the best hyperparameters\n",
        "result = gp_minimize(objective, space, n_calls=50, random_state=42)\n",
        "\n",
        "# Extract the optimal hyperparameters\n",
        "best_C = result.x[0]\n",
        "best_gamma = result.x[1]\n",
        "best_kernel = result.x[2]\n",
        "print(f\"Optimal hyperparameters: C = {best_C}, gamma = {best_gamma}, kernel = {best_kernel}\")\n",
        "\n",
        "# Train the final model with the optimal hyperparameters\n",
        "final_model = SVC(C=best_C, gamma=best_gamma, kernel=best_kernel)\n",
        "final_model.fit(X, y)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = cross_val_score(final_model, X, y, cv=5, scoring='accuracy').mean()\n",
        "print(f\"Cross-validated accuracy of the final model: {accuracy:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiMgD1GA2-xi",
        "outputId": "7b1ca864-3223-47e9-cc30-6325e813c70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal hyperparameters: C = 100.0, gamma = 0.009350485830407594, kernel = poly\n",
            "Cross-validated accuracy of the final model: 0.95217\n"
          ]
        }
      ]
    }
  ]
}