{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattValSE/AutoML2024_Team5/blob/main/AutoML_project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable Automated Machine Learning (LTAT.02.023)\n",
        "## Course project 1\n",
        "Authors:  Kristjan Laid, Mattias Väli and Evely Kirsiaed\n",
        "\n",
        "Description:\n",
        "1. Build and train a baseline, and record the result (train different machine learning algorithms with their default hyperparameters (random forest, decision tree,........etc.) and then select the one that achieves the best performance.).\n",
        "2. Based on the problem at hand, you study the potential pipeline structure, algorithms, or feature transformers at each step and hyperparameter ranges. Use hyperOpt with the potential search space to beat the baseline if possible.\n",
        "\n",
        "Assessment Criteria:\n",
        "1. The project’s code should be available and ready to run if needed.\n",
        "2. Everyone should understand the whole project and be ready to answer any question regarding it, not only the part they contributed to.\n",
        "3. The total assessment time is 15 minutes: 10 minutes for the presentation and 5 minutes for questions.\n",
        "4. You may be interrupted during the presentation for some questions.\n",
        "6. Your presentation should include a dataset description, search space configurations, used baseline, selected pipeline (autoML output), overtime monitoring of the process selection, comparison between the selected and baseline pipelines, statistical test results, and justification for each step.\n",
        "7. Evaluation criteria:\n",
        "\n",
        "a. Correctness of the code - 33% of the mark.\n",
        "\n",
        "b. Completeness of the presentation - 33% of the mark.\n",
        "\n",
        "c. Questions’ answers - 33% of the mark.\n",
        "\n",
        "8. All team members share the same mark for a and b and might get a different mark for c, based on each student’s answers."
      ],
      "metadata": {
        "id": "xf-8INoWYGPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset description\n",
        "todo"
      ],
      "metadata": {
        "id": "4SBPaUHg_HGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model\n"
      ],
      "metadata": {
        "id": "8hibnLOtZ_k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "rLxkhBrTaDAf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYOC8ombzBqX",
        "outputId": "9d96c32c-90f5-4d5c-992d-2f181c4463ac"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (e.g., Iris dataset)\n",
        "data = datasets.load_digits()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "cpTSLI06aId6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "res = dict()\n",
        "for model in [GradientBoostingClassifier(random_state = 42),\n",
        "              RandomForestClassifier(random_state = 42),\n",
        "              XGBClassifier(random_state = 42),\n",
        "              RidgeClassifier(random_state = 42),\n",
        "              LogisticRegression(random_state = 42),\n",
        "              GaussianNB(),\n",
        "              BernoulliNB(),\n",
        "              NearestCentroid(),\n",
        "              KNeighborsClassifier(),\n",
        "              DecisionTreeClassifier(random_state = 42),\n",
        "              SVC()\n",
        "              ]:\n",
        "  # Perform cross-validation and calculate the mean accuracy for each baseline model\n",
        "  accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "  res[model] = accuracy\n",
        "\n",
        "print(\"Mean accuracy for baseline models\")\n",
        "#for el in dict(sorted(res.items(), key=lambda item: item[1], reverse=True)):\n",
        "#  print(f\"Cross-validated accuracy: {res[el]:.5f}, Model: {el}\")\n",
        "\n",
        "sorted_res = dict(sorted(res.items(), key=lambda item: item[1], reverse=True))\n",
        "for model_name, accuracy in sorted_res.items():\n",
        "    print(f\"\\\\item {model_name} - {accuracy * 100:.1f}\\\\%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTdwxGqjaay3",
        "outputId": "07cc74f0-2156-4f60-b6f9-d52665e5a57e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy for baseline models\n",
            "\\item SVC() - 94.9\\%\n",
            "\\item KNeighborsClassifier() - 94.4\\%\n",
            "\\item RandomForestClassifier(random_state=42) - 93.9\\%\n",
            "\\item GradientBoostingClassifier(random_state=42) - 92.2\\%\n",
            "\\item LogisticRegression(random_state=42) - 91.9\\%\n",
            "\\item XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "              gamma=None, grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
            "              num_parallel_tree=None, random_state=42, ...) - 91.8\\%\n",
            "\\item RidgeClassifier(random_state=42) - 88.8\\%\n",
            "\\item NearestCentroid() - 85.6\\%\n",
            "\\item BernoulliNB() - 85.3\\%\n",
            "\\item DecisionTreeClassifier(random_state=42) - 79.0\\%\n",
            "\\item GaussianNB() - 76.9\\%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter optimization"
      ],
      "metadata": {
        "id": "9CwqKKeC84bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter optimization using Hyperopt"
      ],
      "metadata": {
        "id": "7E3bZRfLGWmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Random search"
      ],
      "metadata": {
        "id": "lcykxbyQBUiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from hyperopt.pyll.base import scope"
      ],
      "metadata": {
        "id": "HhwTiZJNGaBi"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the objective function to minimize\n",
        "def objective(params):\n",
        "    # Create the model using parameters from Hyperopt\n",
        "    rf = RandomForestClassifier(n_estimators=int(params['n_estimators']),\n",
        "                                #max_depth=int(params['max_depth']),\n",
        "                                criterion='gini',\n",
        "                              #  max_depth=int(params['max_depth']),\n",
        "                               # min_samples_split=int(params['min_samples_split']),\n",
        "                                #min_samples_leaf=int(params['min_samples_leaf']),\n",
        "                                random_state=42)\n",
        "\n",
        "    # Fit the model on the training set\n",
        "    #rf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    #y_pred = rf.predict(X_valid)\n",
        "\n",
        "    # Compute the Mean Absolute Error (MAE)\n",
        "    #mae = mean_absolute_error(y_valid, y_pred)\n",
        "    accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "    # Return a dictionary with the loss (to minimize) and status\n",
        "    return {'loss': -accuracy,'status': STATUS_OK}\n",
        "\n",
        "# 3. Define the search space for Hyperopt\n",
        "search_space = {\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 30, 250, 5)),\n",
        "  #  'max_depth': scope.int(hp.quniform('max_depth', 2, 20, 1)),  # Integer values between 2 and 20\n",
        "   # 'min_samples_split': scope.int(hp.quniform('min_samples_split', 1, 50, 1)),\n",
        "   # 'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 50, 1))  # Integer values between 1 and 50\n",
        "}\n",
        "\n",
        "\n",
        "trials = Trials()\n",
        "# 5. Run Hyperopt to minimize the objective function\n",
        "best = fmin(fn=objective,                # Objective function\n",
        "            space=search_space,          # Search space\n",
        "            algo=tpe.suggest,            # Tree-structured Parzen Estimator (TPE) algorithm\n",
        "            max_evals=100,               # Number of evaluations\n",
        "            trials=trials,               # Store results\n",
        "            rstate=np.random.default_rng(42))  # Ensure reproducibility with a fixed random seed\n",
        "\n",
        "# 6. Print the best hyperparameters\n",
        "print(\"Best hyperparameters found: \", best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rH8Dr9mGeJ6",
        "outputId": "0ebbb704-5e78-4355-dead-47c611548a11"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 100/100 [00:49<00:00,  2.02trial/s, best loss: -0.9493624264933457]\n",
            "Best hyperparameters found:  {'n_estimators': 210.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train a model with the best hyperparameters on the full training set and evaluate on the test set\n",
        "\n",
        "best_rf = RandomForestClassifier(n_estimators=int(best['n_estimators']),\n",
        "                                 criterion='gini',\n",
        "#                                 max_depth=int(best['max_depth']),\n",
        "                                # min_samples_split=int(best['min_samples_split']),\n",
        "#                                 min_samples_leaf=int(best['min_samples_leaf']),\n",
        "                                 random_state=42)\n",
        "\n",
        "#best_rf = RandomForestClassifier(random_state=42)\n",
        "accuracy = cross_val_score(best_rf, X, y, cv=5, scoring='accuracy') .mean()\n",
        "print(f\"Cross-validated accuracy: {accuracy:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nafXOcD69QsZ",
        "outputId": "d544dcb6-fe8c-4e4c-898d-62b9dfe46879"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy: 0.94104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Grid search"
      ],
      "metadata": {
        "id": "-b0RvrRjBgkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "n_estimators_values = np.arange(30, 250, 20)\n",
        "max_depth_values = np.arange(1, 20, 5)\n",
        "#min_sampl_values = np.arange(1, 20, 5)\n",
        "#min_sampl_leaf = np.arange(1, 20, 5)\n",
        "\n",
        "grid_search_space = {\n",
        "    'n_estimators': hp.choice('n_estimators', n_estimators_values),\n",
        "    'max_depth': hp.choice('max_depth', max_depth_values),  # Grid of fixed values\n",
        "    #'min_samples_split': hp.choice('min_samples_split', min_sampl_values),\n",
        "    #'min_samples_leaf': hp.choice('min_samples_leaf', min_sampl_leaf)  # Integer values between 1 and 50\n",
        "}\n",
        "# Use itertools.product to find the Cartesian product (i.e., all possible combinations)\n",
        "all_combinations = list(itertools.product(n_estimators_values, max_depth_values))#,min_sampl_values,min_sampl_leaf))\n",
        "total_combinations = len(all_combinations)\n",
        "# Perform Grid Search using fmin\n",
        "best_grid = fmin(fn=objective,        # Objective function\n",
        "                 space=grid_search_space,  # Grid search space\n",
        "                 algo=tpe.suggest,    # Still use TPE, but this is effectively Grid Search due to the fixed values\n",
        "                 max_evals=total_combinations,       # Number of evaluations, adjust if necessary\n",
        "                 rstate=np.random.default_rng(42))  # Ensure reproducibility\n",
        "best_grid_rf = RandomForestClassifier(n_estimators=int(best_grid['n_estimators']),\n",
        "                                      max_depth=int(best_grid['max_depth']),\n",
        "                                      #min_samples_split=int(best_grid['min_samples_split']),\n",
        "                                      #min_samples_leaf=int(best_grid['min_samples_leaf']),\n",
        "                                      random_state=42,\n",
        "                                      criterion='gini')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGDg-wphBloF",
        "outputId": "2bf933ac-4ca6-4c7e-e73a-35462379cfa4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 44/44 [00:21<00:00,  2.01trial/s, best loss: -0.9493624264933457]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the actual values from the search space using the indices stored in best_grid\n",
        "best_n_estimators = n_estimators_values[int(best_grid['n_estimators'])]\n",
        "best_max_depth = max_depth_values[int(best_grid['max_depth'])]\n",
        "#best_min_samples_split = min_sampl_values[int(best_grid['min_samples_split'])]\n",
        "#best_min_samples_leaf = min_sampl_leaf[int(best_grid['min_samples_leaf'])]\n",
        "\n",
        "# Now create the RandomForestClassifier with the correct hyperparameter values\n",
        "best_grid_rf = RandomForestClassifier(n_estimators=best_n_estimators,\n",
        "                                      max_depth=best_max_depth,\n",
        "                                      #min_samples_split=best_min_samples_split,\n",
        "                                      #min_samples_leaf=best_min_samples_leaf,\n",
        "                                      random_state=42,\n",
        "                                      criterion='gini')"
      ],
      "metadata": {
        "id": "sxFXBCHRn4Fz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = cross_val_score(best_grid_rf, X, y, cv=5, scoring='accuracy') .mean()\n",
        "print(f\"Cross-validated accuracy: {accuracy:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNrqV-6pDh8o",
        "outputId": "e3aeaa5c-9ca4-4d1b-ef2a-302bced75387"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy: 0.93937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian optimization"
      ],
      "metadata": {
        "id": "LTyJQBSSDu3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "from skopt.space import Real\n",
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NxN73Xy9-J6",
        "outputId": "95422ea4-15b6-413f-ad8c-333c0e59e48a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt.space import Real, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skopt import gp_minimize\n",
        "\n",
        "# Define the SVM model's hyperparameter space\n",
        "space = [\n",
        "    Real(1e-6, 100.0, \"log-uniform\", name='C'),\n",
        "    Real(1e-6, 100.0, \"log-uniform\", name='gamma'),\n",
        "    Categorical(['linear', 'rbf', 'poly', 'sigmoid'], name='kernel')\n",
        "]\n",
        "\n",
        "# Define the objective function to minimize\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    # Create the SVM model with the given hyperparameters\n",
        "    model = SVC(C=params['C'], gamma=params['gamma'], kernel=params['kernel'])\n",
        "\n",
        "    # Perform cross-validation and calculate the mean accuracy\n",
        "    accuracy = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "    # Return the negative accuracy (because we want to minimize the objective)\n",
        "    return -accuracy\n",
        "\n",
        "# Run Bayesian optimization to find the best hyperparameters\n",
        "result = gp_minimize(objective, space, n_calls=50, random_state=42)\n",
        "\n",
        "# Extract the optimal hyperparameters\n",
        "best_C = result.x[0]\n",
        "best_gamma = result.x[1]\n",
        "best_kernel = result.x[2]\n",
        "print(f\"Optimal hyperparameters: C = {best_C}, gamma = {best_gamma}, kernel = {best_kernel}\")\n",
        "\n",
        "# Train the final model with the optimal hyperparameters\n",
        "final_model = SVC(C=best_C, gamma=best_gamma, kernel=best_kernel)\n",
        "final_model.fit(X, y)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = cross_val_score(final_model, X, y, cv=5, scoring='accuracy').mean()\n",
        "print(f\"Cross-validated accuracy of the final model: {accuracy:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiMgD1GA2-xi",
        "outputId": "82e039d5-250b-485e-9e06-b116646d3ce2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal hyperparameters: C = 100.0, gamma = 0.009350485830407594, kernel = poly\n",
            "Cross-validated accuracy of the final model: 0.95217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fAnova"
      ],
      "metadata": {
        "id": "uHs7u0Eb5Rb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna==4.0.0"
      ],
      "metadata": {
        "id": "nKI6lOuozj-C",
        "outputId": "0bc29eb6-3442-4034-aa49-7ad383ce9c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna==4.0.0 in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna==4.0.0) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna==4.0.0) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna==4.0.0) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna==4.0.0) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna==4.0.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from optuna.importance import FanovaImportanceEvaluator\n",
        "from optuna.visualization import plot_param_importances, plot_slice\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "3liEqzfvzlPu"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_digits()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training + validation and test sets (e.g., 80% train + 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Split the training + validation data into separate training and validation sets (e.g., 75% train, 25% val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
        "\n"
      ],
      "metadata": {
        "id": "qo_q5M2V3Wbh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Define objective function\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 10, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
        "        'min_samples_split':trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf':trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "\n",
        "    }\n",
        "\n",
        "    model = RandomForestClassifier(**params, random_state=42)\n",
        "\n",
        "    ## Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "    return mse\n",
        "\n",
        "\n",
        "# Create the Optuna study and optimize\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "importance_evaluator = FanovaImportanceEvaluator()\n",
        "\n",
        "# Calculate hyperparameter importance\n",
        "importance = optuna.importance.get_param_importances(study, evaluator=importance_evaluator)\n",
        "\n",
        "\n",
        "print('Hyperparam importance using fANOVA')\n",
        "for param, score in importance.items():\n",
        "    print(f'Parameter: {param}, Importance: {round(score,4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbivCivloEEf",
        "outputId": "b3aba7bd-d85e-4430-e5a6-432080e132f3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-17 21:18:26,677] A new study created in memory with name: no-name-cee961fc-020f-460c-a612-093afb62c04e\n",
            "[I 2024-11-17 21:18:28,610] Trial 0 finished with value: 1.0888888888888888 and parameters: {'n_estimators': 417, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0888888888888888.\n",
            "[I 2024-11-17 21:18:32,917] Trial 1 finished with value: 0.6666666666666666 and parameters: {'n_estimators': 343, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:35,482] Trial 2 finished with value: 1.15 and parameters: {'n_estimators': 491, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:38,073] Trial 3 finished with value: 0.9333333333333333 and parameters: {'n_estimators': 480, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:40,821] Trial 4 finished with value: 1.0361111111111112 and parameters: {'n_estimators': 459, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:44,692] Trial 5 finished with value: 1.086111111111111 and parameters: {'n_estimators': 499, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:46,539] Trial 6 finished with value: 1.0888888888888888 and parameters: {'n_estimators': 163, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:49,341] Trial 7 finished with value: 0.9361111111111111 and parameters: {'n_estimators': 336, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.6666666666666666.\n",
            "[I 2024-11-17 21:18:51,088] Trial 8 finished with value: 0.425 and parameters: {'n_estimators': 283, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:18:52,844] Trial 9 finished with value: 0.6555555555555556 and parameters: {'n_estimators': 297, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:18:53,644] Trial 10 finished with value: 0.6888888888888889 and parameters: {'n_estimators': 101, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:18:55,166] Trial 11 finished with value: 0.6916666666666667 and parameters: {'n_estimators': 211, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:18:56,761] Trial 12 finished with value: 0.6611111111111111 and parameters: {'n_estimators': 290, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:18:56,977] Trial 13 finished with value: 0.7111111111111111 and parameters: {'n_estimators': 33, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:18:59,375] Trial 14 finished with value: 0.7805555555555556 and parameters: {'n_estimators': 247, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:19:01,019] Trial 15 finished with value: 0.7305555555555555 and parameters: {'n_estimators': 378, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:19:01,963] Trial 16 finished with value: 0.5805555555555556 and parameters: {'n_estimators': 282, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:19:02,606] Trial 17 finished with value: 0.425 and parameters: {'n_estimators': 182, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:19:03,182] Trial 18 finished with value: 0.4361111111111111 and parameters: {'n_estimators': 166, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:19:03,515] Trial 19 finished with value: 0.8138888888888889 and parameters: {'n_estimators': 98, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.425.\n",
            "[I 2024-11-17 21:19:04,226] Trial 20 finished with value: 0.4111111111111111 and parameters: {'n_estimators': 217, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 20 with value: 0.4111111111111111.\n",
            "[I 2024-11-17 21:19:04,957] Trial 21 finished with value: 0.4111111111111111 and parameters: {'n_estimators': 217, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 20 with value: 0.4111111111111111.\n",
            "[I 2024-11-17 21:19:05,719] Trial 22 finished with value: 0.4388888888888889 and parameters: {'n_estimators': 226, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 20 with value: 0.4111111111111111.\n",
            "[I 2024-11-17 21:19:06,150] Trial 23 finished with value: 0.6055555555555555 and parameters: {'n_estimators': 133, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 20 with value: 0.4111111111111111.\n",
            "[I 2024-11-17 21:19:06,870] Trial 24 finished with value: 0.35555555555555557 and parameters: {'n_estimators': 213, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 24 with value: 0.35555555555555557.\n",
            "[I 2024-11-17 21:19:07,566] Trial 25 finished with value: 0.675 and parameters: {'n_estimators': 217, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 24 with value: 0.35555555555555557.\n",
            "[I 2024-11-17 21:19:07,873] Trial 26 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 82, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:08,060] Trial 27 finished with value: 0.7111111111111111 and parameters: {'n_estimators': 54, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:08,131] Trial 28 finished with value: 1.1083333333333334 and parameters: {'n_estimators': 13, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:08,412] Trial 29 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 78, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:08,682] Trial 30 finished with value: 0.6305555555555555 and parameters: {'n_estimators': 75, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:09,130] Trial 31 finished with value: 0.4361111111111111 and parameters: {'n_estimators': 130, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:09,577] Trial 32 finished with value: 0.7555555555555555 and parameters: {'n_estimators': 133, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:09,906] Trial 33 finished with value: 0.2916666666666667 and parameters: {'n_estimators': 91, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:10,120] Trial 34 finished with value: 0.6527777777777778 and parameters: {'n_estimators': 59, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:10,486] Trial 35 finished with value: 0.2916666666666667 and parameters: {'n_estimators': 101, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:10,933] Trial 36 finished with value: 0.6027777777777777 and parameters: {'n_estimators': 101, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:11,061] Trial 37 finished with value: 1.1777777777777778 and parameters: {'n_estimators': 20, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:11,480] Trial 38 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 80, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:11,734] Trial 39 finished with value: 0.7472222222222222 and parameters: {'n_estimators': 48, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:12,113] Trial 40 finished with value: 1.0694444444444444 and parameters: {'n_estimators': 81, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:12,784] Trial 41 finished with value: 0.4361111111111111 and parameters: {'n_estimators': 128, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:13,225] Trial 42 finished with value: 0.44722222222222224 and parameters: {'n_estimators': 84, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:13,832] Trial 43 finished with value: 0.65 and parameters: {'n_estimators': 115, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:14,368] Trial 44 finished with value: 0.36666666666666664 and parameters: {'n_estimators': 154, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:14,530] Trial 45 finished with value: 0.7166666666666667 and parameters: {'n_estimators': 40, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:15,185] Trial 46 finished with value: 0.3 and parameters: {'n_estimators': 183, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:15,444] Trial 47 finished with value: 0.6972222222222222 and parameters: {'n_estimators': 72, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:16,787] Trial 48 finished with value: 0.6694444444444444 and parameters: {'n_estimators': 435, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:16,923] Trial 49 finished with value: 0.38333333333333336 and parameters: {'n_estimators': 31, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:17,321] Trial 50 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 106, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:17,889] Trial 51 finished with value: 0.4777777777777778 and parameters: {'n_estimators': 155, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:18,532] Trial 52 finished with value: 0.4722222222222222 and parameters: {'n_estimators': 185, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:18,786] Trial 53 finished with value: 0.45 and parameters: {'n_estimators': 69, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:19,424] Trial 54 finished with value: 0.5777777777777777 and parameters: {'n_estimators': 180, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:19,766] Trial 55 finished with value: 0.2916666666666667 and parameters: {'n_estimators': 95, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:20,094] Trial 56 finished with value: 0.5694444444444444 and parameters: {'n_estimators': 89, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:20,503] Trial 57 finished with value: 0.625 and parameters: {'n_estimators': 119, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:21,519] Trial 58 finished with value: 0.8722222222222222 and parameters: {'n_estimators': 333, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:21,988] Trial 59 finished with value: 0.6583333333333333 and parameters: {'n_estimators': 143, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:22,201] Trial 60 finished with value: 0.3416666666666667 and parameters: {'n_estimators': 51, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:22,621] Trial 61 finished with value: 0.45 and parameters: {'n_estimators': 113, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:22,954] Trial 62 finished with value: 0.5888888888888889 and parameters: {'n_estimators': 90, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:23,192] Trial 63 finished with value: 0.2833333333333333 and parameters: {'n_estimators': 60, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:23,439] Trial 64 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 64, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:23,681] Trial 65 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 62, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:23,810] Trial 66 finished with value: 1.1055555555555556 and parameters: {'n_estimators': 24, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:24,160] Trial 67 finished with value: 0.6555555555555556 and parameters: {'n_estimators': 61, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:24,384] Trial 68 finished with value: 0.48333333333333334 and parameters: {'n_estimators': 34, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:24,500] Trial 69 finished with value: 0.9833333333333333 and parameters: {'n_estimators': 11, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:24,863] Trial 70 finished with value: 0.3416666666666667 and parameters: {'n_estimators': 66, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:25,167] Trial 71 finished with value: 0.3527777777777778 and parameters: {'n_estimators': 54, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:25,446] Trial 72 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 43, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 26 with value: 0.2722222222222222.\n",
            "[I 2024-11-17 21:19:25,722] Trial 73 finished with value: 0.20277777777777778 and parameters: {'n_estimators': 44, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:25,920] Trial 74 finished with value: 0.5138888888888888 and parameters: {'n_estimators': 29, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:26,189] Trial 75 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 43, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:26,461] Trial 76 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 43, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:26,574] Trial 77 finished with value: 0.6583333333333333 and parameters: {'n_estimators': 10, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:27,020] Trial 78 finished with value: 0.5805555555555556 and parameters: {'n_estimators': 81, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:27,177] Trial 79 finished with value: 0.7777777777777778 and parameters: {'n_estimators': 37, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:27,373] Trial 80 finished with value: 0.65 and parameters: {'n_estimators': 45, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:27,634] Trial 81 finished with value: 0.3416666666666667 and parameters: {'n_estimators': 65, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:27,915] Trial 82 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 77, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:28,205] Trial 83 finished with value: 0.5916666666666667 and parameters: {'n_estimators': 76, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:28,333] Trial 84 finished with value: 0.6083333333333333 and parameters: {'n_estimators': 27, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:28,729] Trial 85 finished with value: 0.7416666666666667 and parameters: {'n_estimators': 110, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:28,922] Trial 86 finished with value: 0.3416666666666667 and parameters: {'n_estimators': 49, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:29,781] Trial 87 finished with value: 0.7805555555555556 and parameters: {'n_estimators': 261, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:30,086] Trial 88 finished with value: 0.24722222222222223 and parameters: {'n_estimators': 82, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:30,547] Trial 89 finished with value: 0.46944444444444444 and parameters: {'n_estimators': 123, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:30,907] Trial 90 finished with value: 0.425 and parameters: {'n_estimators': 99, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:31,188] Trial 91 finished with value: 0.2722222222222222 and parameters: {'n_estimators': 76, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:31,518] Trial 92 finished with value: 0.35 and parameters: {'n_estimators': 87, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:31,774] Trial 93 finished with value: 0.48055555555555557 and parameters: {'n_estimators': 66, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:33,028] Trial 94 finished with value: 0.42777777777777776 and parameters: {'n_estimators': 378, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:33,207] Trial 95 finished with value: 0.5611111111111111 and parameters: {'n_estimators': 42, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:33,314] Trial 96 finished with value: 0.4777777777777778 and parameters: {'n_estimators': 19, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:33,549] Trial 97 finished with value: 0.3527777777777778 and parameters: {'n_estimators': 58, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:33,833] Trial 98 finished with value: 0.625 and parameters: {'n_estimators': 80, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 73 with value: 0.20277777777777778.\n",
            "[I 2024-11-17 21:19:34,187] Trial 99 finished with value: 0.8111111111111111 and parameters: {'n_estimators': 106, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 73 with value: 0.20277777777777778.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparam importance using fANOVA\n",
            "Parameter: min_samples_leaf, Importance: 0.8869\n",
            "Parameter: max_depth, Importance: 0.0574\n",
            "Parameter: min_samples_split, Importance: 0.0289\n",
            "Parameter: n_estimators, Importance: 0.0267\n"
          ]
        }
      ]
    }
  ]
}